[["index.html", "Corpus Phonetics Tutorial 1 Introduction", " Corpus Phonetics Tutorial Eleanor Chodroff Updated: 2022-09-30 1 Introduction Corpus phonetics has become an increasingly popular method of research in linguistic analysis. With advances in speech technology and computational power, large scale processing of speech data has become a viable technique. A fair number of researchers have exploited these methods, yet these techniques still remain elusive for many. In the words of Mark Liberman, there has been “surprisingly little change in style and scale of [phonetic] research” from 1966 on, implying that the field still relies on small sample sizes of speech data (2009). While “big data” phonetics is not the be-all and end-all of phonetic research, larger sample sizes ensure more statistically sound conclusions about phonetic values in an individual or population. Furthermore, corpus research is not synonymous with big data. Rather, corpus phonetics describes a method of processing speech data with advantages primarily gained in its computational power (relation to big data) and efficiency. The methods and tools developed for corpus phonetics are based on engineering algorithms primarily from automatic speech recognition (ASR), as well as simple programming for data manipulation. This tutorial aims to bring some of these tools to the non-engineer, and specifically to the speech scientist. Acoustic analysis programs such as Praat, MATLAB, and R (check out the tuneR and multitaper packages) are already capable of large scale phonetic measurement via their respective scripting languages. While the tutorial covers some phonetic processing in Praat, the primary aim is to introduce supplementary tools to phonetic processing. These tools are based on concepts and algorithms from automatic speech recognition, which allow for automatic alignment of phonetic boundaries to the speech signal. In particular, the tutorial currently covers various tools from the Kaldi Automatic Speech Recognition Toolki, the Montreal Forced Aligner (MFA v2), AutoVOT, and bash shell usage. You can also find additional resources for Praat scripting, additional corpus phonetic tools, and legacy tutorial pages for MFA version 1, FAVE-align, and the Penn Phonetics Lab Forced Aligner in the section on Other Resources. Kaldi is an automatic speech recognition toolkit that provides the infrastructure to build personalized acoustic models and forced alignment systems. Acoustic models are the statistical representations of each phoneme’s acoustic information. The “personalized” component means that this system is capable of modeling any corpus of speech, be it British English, Southern American English, Hungarian, or Korean. It additionally houses many speech processing algorithms, which may be of use to the speech scientist. This tutorial will cover acoustic model training and forced alignment in Kaldi; however, the toolkit as a whole provides exceptional potential for phonetic research. “Forced alignment” is the automatic synchronization of a sequence of phones with an audio file. This process employs acoustic models of the sounds of a language, along with a pronunciation lexicon which provides a canonical mapping from orthographic words to sequences of phones. Forced alignment greatly expedites data processing and phonetic measurement. Kaldi and the Montreal Forced Aligner are all capable of forced alignment, but with varying degrees of flexibility with respect to the input speech. Finally, AutoVOT is an automatic voice onset time (VOT) measurement tool that demarcates the burst release and vocalic onset of word-initial, prevocalic stop consonants. The tutorial assumes basic familiarity with Praat, as well as a Mac operating system, primarily for the default bash/Unix shell in the Terminal application. If using a PC, I recommend downloading Cygwin for running bash/Unix commands, though the Montreal Forced Aligner will also work fine on the downloaded conda console. For AutoVOT and the Penn Forced Aligner, most of the Unix commands are provided in the tutorial itself. While I try to provide as many of the commands as possible, Kaldi requires more fluency in shell scripting. If you have not used the Terminal application before, I recommend looking over some basic Unix commands online (Google or the section on Bash Shell Basics). If you’d like more info on Corpus Phonetics as I view it, you can check out some slides I’ve presented on the topic here. A brief overview to my quick and dirty approach to Praat scripting can also be found in the slides here. Citations for each of the programs can be found below: Kaldi Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Motlicek, P., Qian, Y., Schwartz, P., Silovsky, J., Stemmer, G., &amp; Vesely, K. (2011). The Kaldi speech recognition toolkit. In IEEE 2011 Workshop on ASRU. @INPROCEEDINGS{ Povey_ASRU2011, author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel}, keywords = {ASR, Automatic Speech Recognition, GMM, HTK, SGMM}, month = dec, title = {The Kaldi Speech Recognition Toolkit}, booktitle = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding}, year = {2011}, publisher = {IEEE Signal Processing Society}, location = {Hilton Waikoloa Village, Big Island, Hawaii, US}, note = {IEEE Catalog No.: CFP11SRW-USB}, } Montreal Forced Aligner McAuliffe, Michael, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger (2017). Montreal Forced Aligner [Computer program]. Version 0.9.0, retrieved 17 January 2017 from http://montrealcorpustools.github.io/Montreal-Forced-Aligner/. AutoVOT Keshet, J., Sonderegger, M., Knowles, T. (2014). AutoVOT: A tool for automatic measurement of voice onset time using discriminative structured prediction [Computer program]. Version 0.91, retrieved August 2014 from https://github.com/mlml/autovot/. This tutorial Chodroff, E. (2018). Corpus phonetics tutorial. arXiv preprint arXiv:1811.05553. https://arxiv.org/pdf/1811.05553.pdf. "],["kaldi.html", "2 Kaldi", " 2 Kaldi Take me to the full Kaldi ASR Tutorial. What is Kaldi? Kaldi is a state-of-the-art automatic speech recognition (ASR) toolkit, containing almost any algorithm currently used in ASR systems. It also contains recipes for training your own acoustic models on commonly used speech corpora such as the Wall Street Journal Corpus, TIMIT, and more. These recipes can also serve as a template for training acoustic models on your own speech data. What are acoustic models? Acoustic models are the statistical representations of a phoneme’s acoustic information. A phoneme here represents a member of the set of speech sounds in a language. N.B., this use of the term ‘phoneme’ only loosely corresponds to the linguistic use of the term ‘phoneme’. The acoustic models are created by training the models on acoustic features from labeled data, such as the Wall Street Journal Corpus, TIMIT, or any other transcribed speech corpus. There are many ways these can be trained, and the tutorial will try to cover some of the more standard methods. Acoustic models are necessary not only for automatic speech recognition, but also for forced alignment. Kaldi provides tremendous flexibility and power in training your own acoustic models and forced alignment system. The following tutorial covers a general recipe for training on your own data. This part of the tutorial assumes more familiarity with the terminal; you will also be much better off if you can program basic text manipulations. "],["montreal-forced-aligner.html", "3 Montreal Forced Aligner 3.1 Overview 3.2 Installation 3.3 Running the aligner 3.4 File preparation 3.5 Obtaining acoustic models 3.6 Obtaining dictionaries 3.7 Tips and tricks", " 3 Montreal Forced Aligner 3.1 Overview The Montreal Forced Aligner is a forced alignment system with acoustic models built using the Kaldi ASR toolkit. A major highlight of this system is the availability of pretrained acoustic models and grapheme-to-phoneme models for a wide variety of languages, as well as the ability to train acoustic and grapheme-to-phoneme models to any new dataset you might have. It also uses advanced techniques for training and aligning speech data (Kaldi) with a full suite of training and speaker adaptation algorithms. The basic acoustic model recipe uses the traditional GMM-HMM framework, starting with monophone models, then triphone models (which allow for context sensitivity, read: coarticulation), and some transformations and speaker adaptation along the way. You can check out more regarding the recipes in the MFA user guide and reference guide; for an overview to a standard training recipe, check out this Kaldi tutorial. As a forced alignment system, the Montreal Forced Aligner will time-align a transcript to a corresponding audio file at the phone and word levels provided there exist a set of pretrained acoustic models and a pronunciation dictionary (a.k.a. lexicon) of the words in the transcript with their canonical phonetic pronunciation(s). The current MFA download contains a suite of mfa commands that allow you to do everything from basic forced alignment to grapheme-to-phoneme conversion to automatic speech recognition. In this tutorial, we’ll be focusing mostly on those commands relating to forced alignment and a side of grapheme-to-phoneme conversion. Very generally, the procedure for forced alignment is as follows: Prep audio file(s) Prep transcript(s) (Praat TextGrids or .lab/.txt files) Obtain a pronunciation dictionary Obtain an acoustic model Create an input folder that contains the audio files and transcripts Create an empty output folder Run the mfa align command 3.2 Installation You can find the streamlined installation instructions on the main MFA installation page. The majority of users will be following the “All platforms” instructions. As listed there, you will need to download Miniconda or Anaconda. If you’re trying to decide between the two, I’d probably recommend Miniconda. Compared to Anaconda, Miniconda is smaller, and has a slightly higher success rate for installation. Conda is a package and environment management system. If you’re familiar with R, the package management system is similar in concept to the R CRAN server. Once installed, you can import sets of packages via the conda commands, similar to how you might run install.packages in R. The environment manager aspect of conda is fairly unique, and essentially creates a small environment on your computer for the collection of imported packages (here the MFA suite) to run. The primary advantage is that you don’t have to worry about conflicting versions of the same code package on your computer; the environment will be a bubble in your computer with the correct package versions for the MFA suite. Once you’ve installed Miniconda/Anaconda, you’ll run the following line of code in the command line. For Mac users, the command line will be the terminal or a downloaded equivalent. For Windows users, I believe you will run this directly in the Miniconda console. conda create -n aligner -c conda-forge montreal-forced-aligner The -n flag here refers to the name of the environment. If you want to test a new version of the aligner, but aren’t ready to overwrite your old working version of the aligner, you can create a new environment using a different name after the -n flag. See @ref(Tips and tricks) for more on conda environments. If it’s worked, you should see something like: # To activate this environment, use # # $ conda activate aligner # # To deactivate an active environment, use # # $ conda deactivate And with that, the final step is activating the aligner. You will need to re-activate the aligner every time you open a new shell/command line window. You should be able to see which environment you have open in the parentheses before each shell prompt. For example, mine looks like: (base) Eleanors-iPro:Documents eleanorchodroff$ conda activate aligner (aligner) Eleanors-iPro:Documents eleanorchodroff$ You can run the aligner from any location on the computer as long as you’re in the aligner environment on the command line. NB: The Montreal Forced Aligner is now located in the Documents/MFA folder. All acoustic models, dictionaries, temporary alignment or training files, etc. will be stored in this folder. If you are ever confused or curious about what is happening with the aligner, just poke around this folder. 3.3 Running the aligner To run the aligner, we’ll need to follow this procedure: Prep audio file(s) Prep transcript(s) (Praat TextGrids or .lab/.txt files) Obtain a pronunciation dictionary Obtain an acoustic model Create an input folder that contains the audio files and transcripts Create an empty output folder Run the align command In this section, I will give a quick example of how this works using a hypothetical example of aligning American English. Through this, you’ll see the primary mfa commands that you’ll need for alignment. Then in the following sections, I’ll go into more detail about each of these steps. For the first steps of prepping the audio files and prepping the transcripts, I’ll assume we’re working with a wav file and a Praat TextGrid that has a single interval tier with utterances transcribed in English. We can have multiple intervals per TextGrid. The wav file and TextGrid must have the same name. After we have the audio files and transcript TextGrids, we’ll place them in an input folder. For the sake of this tutorial, this input folder will be called Documents/input_english/. We will also need to create an empty output folder. I will call this output folder Documents/output_english. We can then obtain the acoustic model from the internet (yes, you’ll have to be online the first time you do this) using the following command: mfa model download acoustic english_us_arpa (You can also use mfa models download acoustic english_us_arpa.) And we can obtain the dictionary from the internet using this command: mfa model download dictionary english_us_arpa (You can also use mfa models download dictionary english_us_arpa.) The dictionary and acoustic model can now be found in their respective Documents/MFA/pretrained_models/ folders. Once we have the dictionary, acoustic model, wav files and TextGrids in their input folder, and an empty output folder, then we’re ready to run the aligner! The alignment command is called align and takes 4 arguments: path to the input folder name of the acoustic model (in the Documents/MFA/pretrained_models/acoustic/ folder) / path to the acoustic model if it’s elsewhere on the computer name of the dictionary (in the Documents/MFA/pretrained_models/dictionary/ folder) / path to the acoustic model if it’s elsewhere on the computer path to the output folder I always add the optional --clean flag as this “cleans” out any old temporary files created in a previous run. (If you’re anything like me, you might find yourself re-running the aligner a few times, and with the same input filename. The aligner won’t actually re-run properly unless you clear out the old files.) Important: You will need to scroll across to see the whole line of code. mfa align --clean /Users/Eleanor/Documents/input_english/ english_us_arpa english_us_arpa /Users/Eleanor/Documents/output_english/ And if everything worked appropriately, your aligned TextGrids should now be in the Documents/output_english/ folder! 3.4 File preparation 3.4.1 Audio files The Montreal Forced Aligner is incredibly robust to audio files of differing formats, sampling rates and channels. You should not have to do much prep, but note that whatever you feed the system will be converted to be a wav file with a sampling rate of 16 kHz with a single (mono) channel unless otherwise specified (see Feature Configuration. For the record, I have not yet tried any other file format except for wav files, so I’m not yet aware of potential issues that might arise there. 3.4.2 Transcripts The MFA can take as input either a Praat TextGrid or a .lab or .txt file. I have worked most extensively with the TextGrid input, so I’ll describe those details here. As for .lab and .txt input, I believe this method only works when the transcript is pasted in as a single line. In other words, I don’t think it can handle time stamps for utterance start and end times. 3.4.3 Filenames The filename of the wav file and its corresponding transcript must be identical except for the extension (.wav or .TextGrid). If you have multiple speakers in the alignment or training, you can actually implement some degree of speaker/channel adaptation. You can do this either by placing speaker-specific files in its own subfolder within the input folder, or by adding an optional argument to the mfa align command to use the first n characters of the filename. If you go forward with this, it helps to have the speaker ID as the prefix to the filenamex. For example: spkr01_utt1.wav, spkr01_utt1.TextGrid spkr01_utt2.wav, spkr01_utt2.TextGrid spkr02_utt1.wav, spkr02_utt1.TextGrid spkr02_utt2.wav, spkr02_utt2.TextGrid spkr03_utt1.wav, spkr03_utt1.TextGrid spkr03_utt2.wav, spkr03_utt2.TextGrid spkr04_utt1.wav, spkr04_utt1.TextGrid spkr04_utt2.wav, spkr04_utt2.TextGrid In this case, we can then tell the aligner to use the first 6 characters as the speaker information. The -s flag stands for speaker characters. mfa align -s 6 --clean /Users/Eleanor/input_english/ english_us_arpa english_us_arpa /Users/Eleanor/output_english/ Alternatively: mfa align --speaker_characters 6 --clean /Users/Eleanor/input_english/ english_us_arpa english_us_arpa /Users/Eleanor/output_english/ 3.4.4 Input and output folders I would recommend creating a special input folder that houses a copy of your audio files and TextGrid transcripts. In case something goes wrong, you won’t be messing up the raw data. You can place this folder basically anywhere on your computer, and you can call this whatever you want. You will also need to create an empty output folder. I recommend making sure this is empty each time you run the aligner as the aligner does not overwrite any existing files. You can place this folder basically anywhere on your computer, and you can call this whatever you want; however, you may not re-use the input folder as the output folder. 3.5 Obtaining acoustic models 3.5.1 Download an acoustic model Pretrained acoustic models for several languages can be downloaded directly using the command line interface. This is the master list of acoustic models available for download. 3.5.2 Train an acoustic model You can also train an acoustic model yourself directly on the data you’re working on. You do need a fair amount of data to get reasonable alignments out. If an existing acoustic model already exists, it might be worth simply using that directly on your data or you could try adapting the model to your data. There are many cases, however, when training a new acoustic model is the best path forward. The mfa train command takes 3 arguments: path to the input folder with the audio files and utterance-level transcripts (TextGrids) name of the pronunciation dictionary in the pretrained_models folder or path to the pronunciation dictionary path to the output folder (it still runs an alignment at the end) In other words, mfa train has the same syntax and requirements as the mfa align command, but without the acoustic model specification. mfa train --clean ~/Documents/input_spanish ~/Documents/talnupf_spanish.txt ~/Documents/output_spanish_textgrids Once again, I’m using the --clean flag just in case I need to clean out an old folder in Documents/MFA. If you would like to reuse the acoustic model, you can find all of the trained models (there are several – one for each layer of training) in the associated folder in Documents/MFA/ and any folder ending in _ali. I would recommend using the acoustic model from the last training and alignment pass, which is the sat2_ali folder. sat2_ali stands for the second round of Speaker Adaptive Training — alignment pass. The mfa model save acoustic command takes one required argument, which is the path to the model you want to save, and an optional (but highly recommended) argument which is the new name of the acoustic model. The optional argument is specified after the --name flag: mfa model save acoustic --name guarani_cv ~/Documents/MFA/prep_validated_guarani/sat2_ali/acoustic_model.zip 3.6 Obtaining dictionaries The pronunciation dictionary must be a two column text file with a list of words on the left-hand side and the phonetic pronunciation(s) on the right-hand side. Each word should be separated from its phonetic pronunciation by a tab, and each phone in the phonetic pronunciation should be separated by a space. Many-to-many mappings between words and pronunciations are permitted. In fact, you can even add pronunciation probabilities to the dictionary, but I have not yet tried this! One important point: the phone set in your dictionary must match that used in the acoustic models and the orthography must match that in the transcripts. There are a few options for obtaining a pronunciation dictionary: 3.6.1 Download a dictionary This is the master list of pre-existing pronunciation dictionaries available through the MFA. Click on the dictionary of interest, and if you scroll to the bottom of the page, it will tell you the name to type in the mfa model download dictionary command. mfa model download dictionary spanish_latin_america_mfa NB: you must add any missing words in your corpus manually, or train a G2P model to handle these cases. 3.6.2 Generate a dictionary using a G2P model Grapheme-to-phoneme (G2P) models automatically convert the orthographic words in your corpus to the most likely phonetic pronunciation. How exactly it does this depends a lot on the type of model and its training data. The MFA has a handful of pretrained G2P models that you can download. This is the master list of G2P models available for download. mfa model download g2p bulgarian_mfa You’ll then use the mfa g2p command to generate the phonetic transcriptions from the submitted orthographic forms and the g2p model. The mfa g2p command takes 3 arguments: name of g2p model (in Documents/MFA/pretrained_models/g2p/) path to the TextGrids or transcripts in your corpus path to where the new pronunciation dictionary should go mfa g2p --clean bulgarian_mfa ~/Desktop/romanian/TextGrids_with_new_words/ ~/Desktop/new_bulgarian_dictionary.txt You can also use an external resource like Epitran or XPF to generate a dictionary for you. These are both rule-based G2P systems built by linguists; these systems work to varying degrees of success. In all cases, you’re best off checking the output. Remember that the phone set in the dictionary must entirely match the phone set in the acoustic model. 3.6.3 Train a G2P model You can also train a G2P model on an existing pronunciation dictionary. Once you’ve trained the G2P model, you’ll need to jump back up to the generate dictionary instructions just above. This might be useful in cases when you have many unlisted orthographic forms that are still in need of a phonetic transcription. The mfa train_g2p command has 2 arguments: path to the training data pronunciation lexicon path to where the trained G2P model should go mfa train_g2p ~/Desktop/romanian/romanian_lexicon.txt ~/Desktop/romanian/romanian_g2p 3.6.4 Create the dictionary by hand This one is self-explanatory, but once again, make sure to use the same phone set as the acoustic models. 3.7 Tips and tricks Add -h after any command to get a help screen that will also display its argument structure. For example: mfa align -h mfa train -h Use the --clean flag each time you run the align command Don’t forget to activate the aligner Make sure the output folder is empty each time you run the align command The input and output folders must be different folders Many users have trouble reading/writing files to the Desktop folder. If you’re having issues using the Desktop, just switch to a different folder like Documents, or Google how to change the read/write permissions on your Desktop folder And a little more on conda: If you have created multiple environments, you can list all of your environments using the following command: conda info --envs You can delete an environment with the following command, where ENV_NAME is the name of the environment, like aligner. Make sure that you have deactivated the environment before deleting it. conda deactivate conda env remove -n ENV_NAME You can inspect the details of any local acoustic model or dictionary using the mfa model inspect commands. One of the important outputs of this is the assumed phone set: mfa model inspect acoustic english_us_arpa mfa model inspect dictionary english_us_arpa You can get a list of the local acoustic models and dictionaries in your Documents/MFA/pretrained_models/ folders using the mfa model list commands: mfa model list acoustic mfa model list dictionary And finally, there is even more to the MFA than just what I’ve put here! Definitely poke around the MFA user guide to learn more and find special functions that might make your workflow even easier. "],["autovot.html", "4 AutoVOT 4.1 Overview 4.2 Simple recipe 4.3 Full recipe", " 4 AutoVOT 4.1 Overview What does AutoVOT do? AutoVOT takes a sound file and Praat TextGrid marked with the locations of word-initial, prevocalic stop consonants and creates a new tier with boundaries marking the stop consonant burst release and following vocalic onset. With these boundaries, positive VOT can be measured efficiently using a standard acoustic analysis program such as Praat. What does it include? AutoVOT can be used via Praat directly or via the command line. The system comes with pre-trained acoustic models/classifiers for American English and British English word-initial, prevocalic VOTs. It also offers the option to train your own acoustic models from labeled training data. As the online tutorial is extremely helpful, I will just offer a few tips from experience. You will first need to visit the online tutorial for the prerequisites and basic installation. Online tutorial and download: https://github.com/mlml/autovot/ Do follow the command line installation instructions closely. Note that there are several dependencies that you might have to download separately. Make sure to do this! I was able to follow their instructions up until the pip -r \"requirements.txt\" command. I did not have pip installed on my computer, so I needed to install that separately. You might also run into this with pip and possibly also git. At least for pip, I solved the problem in the following way: First: install pip by typing the following into the command line (must be connected to the internet). sudo easy_install pip sudo pip install --upgrade pip Second: even after installing pip, I had trouble running their command of pip -r \"requirements.txt\", so I installed each package separately in the following way. These should again be typed into the command line (bash). pip install numpy pip install future pip install scipy pip install textgrid pip install argparse 4.2 Simple recipe To run AutoVOT, you will need the following: a TextGrid with an interval tier for the VOT “windows of analysis” corresponding 16kHz mono wav file a text file containing the names and locations (the path) for the TextGrids to be processed a text file containing the names and locations (the path) for the wav files to be processed The TextGrid should contain at least one tier with an interval for every VOT to be processed. If you would like to run AutoVOT in one go, the label of the intervals to be processed should be the same (e.g., “vot” or “stop”). If you would like to run AutoVOT separately for each stop consonant (e.g., once for “p”, once for “t”, and once for “k”; there are good reasons for doing this), then see the full recipe below. The windows of analysis are the intervals surrounding the word-initial stop consonant that AutoVOT should process. You should make sure that the analysis window provides enough space before the start and after the end for the system to be able to identify the stop release (or burst) and the onset of voicing in the following segment.) As mentioned in the list above, the wav file needs to have a sampling rate of 16 kHz and single channel. The text files containing the paths and filenames to the TextGrids and wav files to be processed can be created manually or automatically. If you’d like to create them automatically, see step 4 in the Full Recipe section. The TextGrids and wav files need to be in the same order, and each filename needs to be on its own line. These two files need to be placed in ~/autovot/experiments/config/. Once you’ve set up your TextGrids, wav files, and have created the lists of the filenames + paths, you should be ready to run AutoVOT. Before you run AutoVOT, make sure to navigate to the experiments folder and set the path to your autovot/autovot/bin folder. You’ll then run auto_vot_decode.py from the experiments folder with the following arguments: –window_tier name of the tier with your vot windows –window_mark name of the label in your vot window intervals –min_vot_length minimum vot length allowed in milliseconds location of your list of wav files location of your list of TextGrids name of model (I typically use the model I specified below which is also located in your autovot/autovot/bin/models/ folder) Example code for a voiceless stop (periodic reminder that backslashes shouldn’t be copied; these are just there as line breaks): cd experiments export PATH=$PATH:/Users/Eleanor/autovot/autovot/bin auto_vot_decode.py --window_tier vot --window_mark P --min_vot_length 15 \\ config/ListWavFiles.txt config/ListTextGrids.txt \\ /Users/Eleanor/autovot/autovot/bin/models/vot_predictor.amanda.max_num_instances_1000.model 4.3 Full recipe The recipe assumes that you have TextGrids containing the phone and word level transcriptions (e.g., Penn Forced Aligner output)! To run AutoVOT, the word-initial, prevocalic stop consonants need to be located and given a “window of analysis” in the TextGrid. The window of analysis is the interval surrounding the stop consonant that AutoVOT will process. This part of the tutorial will cover one method for accomplishing this. This part of the tutorial is a bit old, and the scripts are somewhat hacky! :) Hopefully they still work! Create a list of word-initial, prevocalic (CV) words in your transcript There are many ways to accomplish this; the following is just one suggested way using some Unix and the P2FA dictionary. The following bash commands take multiple transcripts with the naming structure S001.txt, S002.txt, etc. as input, concatenates them, removes punctuation, puts each word on a new line, strips end of line characters, converts lowercase to uppercase, then removes duplicate words. The output of this is returned in the text file fulltranscript_words.txt. If you have a single transcript, you can replace S*[0-9].txt with the name of your transcript. Before running this code, you must direct the terminal (cd) to the directory housing your full transcript (fulltranscript.txt). cat S*[0-9].txt | tr -d &#39;[:punct:]&#39; | tr &#39; &#39; &#39;\\n&#39; | sed &#39;/^$/d&#39; | tr &#39;[a-z]&#39; &#39;[A-Z]&#39; | sort | uniq &gt; transcript.txt matchText.py matchText.py takes as input the CMU Pronouncing Dictionary with a ‘.txt’ extension. It identifies which of the words in your transcript begin with stop consonants in prevocalic position. These words are stored in the text file wordList.txt. You will need to modify the path locations and possibly the regular expression in matchText.py. # run matchText.py python matchText.py Find start and end times for words on wordList.txt in TextGrids with phone- and word- level boundaries findWords.praat findWords.praat takes as input wordList.txt(https://www.eleanorchodroff.com/tutorial/scripts/wordList.txt){target=“_blank”} and returns the start and end times of matching words in the audio file. This is stored in the text file CVWordLocations.txt(https://www.eleanorchodroff.com/tutorial/scripts/CVWordLocations.txt){target=“_blank”}. You will need to modify the source and destination paths in findWords.praat. Verify that the regular expression in ‘Create Strings as file list…’ will work for your setup. Create AutoVOT intervals on a new tier in your Praat TextGrid makeAutoVOTTextGrids.praat This script takes as input the text file CVWordLocations.txt and the Penn Forced Aligner TextGrids. It adds an interval tier ‘vot’ and renames the TextGrids to filename_allauto.TextGrid. Because all stop consonants are word-initial, the start of the word is assumed to be the start of the stop consonant. The end of the stop consonant is identified using the interval on the phone tier that aligns with the start of the word. Those boundaries are then used to create an AutoVOT check interval on the new vot tier. If the stop consonant begins with PTK, then those boundaries are extended 31 ms in both directions. If the stop consonant begins with BDG, then the boundaries are extended 11 ms in both directions. The extra, odd millisecond is to reduce the chance of placing a boundary where one already exists. Note that the P2FA boundaries can only be placed at 10 ms intervals, so adding time at a factor of 10 ms results in overlapping/identical boundaries. The interval text is then set with the phone name (PTKBDG) after all boundaries have been created. This ensures that overlapping intervals will not be a problem. The output should look something like this: Example TextGrid for AutoVOT The different labels in the ‘vot’ tier will have some consequences when running AutoVOT. The AutoVOT analysis depends on a consistent label for the intervals it needs to check. Since there are six different interval labels [PTKBDG], AutoVOT will need to be run six different times. For me, this is preferable, as I then know exactly which stop consonant was measured. For some, however, this may be too tedious. In that case, I still highly recommend running the voiced and voiceless stop consonants separately. Make a list of your TextGrids and wav files and move them the lists to autovot/experiments/config The code below will generate a textfile located in the correct autovot folder that contains a list of the TextGrids with their path (PWD is the command that prints this). When generating these lists, the terminal must be in the directory that contains the TextGrids and wav files, respectively. cd experiment/myTextGrids ls -d -1 $PWD/*_vot.TextGrid &gt; ~/autovot/experiments/config/ListTextGrids.txt cd experiment/myWavFiles ls -d -1 $PWD/*.wav &gt; ~/autovot/experiments/config/ListWavFiles.txt Run AutoVOT from the terminal on one stop consonant Modify the arguments to auto_vot_decode.py: window_mark should be set to the stop consonant to be analyzed. We recommend setting —min_vot_length to 4 (ms) for voiced stops and 15 (ms) for voiceless stops. After each stop consonant, there is a post-processing step. Make sure to do that to avoid overwriting the AutoVOT output (see step 6)! Example code for a voiceless stop: cd experiments export PATH=$PATH:/Users/Eleanor/autovot/autovot/bin auto_vot_decode.py --window_tier vot --window_mark P --min_vot_length 15 \\ config/ListWavFiles.txt config/ListTextGrids.txt \\ /Users/Eleanor/autovot/autovot/bin/models/vot_predictor.amanda.max_num_instances_1000.model Example code for a voiced stop: cd experiments export PATH=$PATH:/Users/Eleanor/autovot/autovot/bin auto_vot_decode.py --window_tier vot --window_mark B --min_vot_length 4 \\ config/ListWavFiles.txt config/ListTextGrids.txt \\ /Users/Eleanor/autovot/autovot/bin/models/vot_predictor.amanda.max_num_instances_1000.model The path (export PATH=$PATH command) should always be set from the experiments directory before running AutoVOT. I recommend the above argument specification, but the structure can also be modified to suit your particular dataset. The arguments in the AutoVOT decode command are listed here: –window_tier This refers to the TextGrid tier that contains the intervals to check (or windows of analysis). The current procedure has called this tier ‘vot’ –window_mark This refers to the label of the interval to check. The current procedure has six different labels [PTKBDG], so this command will need to be run six times, once for each of these labels. After each run, the output tier will need to be renamed so that it is not overwritten. –min_vot_length This refers to the minimum allowed VOT length. I would recommend 15ms for voiceless stops and 4ms for voiced stops, but this can be modified. It should be noted, however, that performance degrades on the voiceless stop measurements if the minimum VOT is too low. (This is why I recommend running AutoVOT separately for the voiced and voiceless stops.) Path from experiments to the list of wav files Path from experiments to the list of TextGrids Path to AutoVOT classifier The default classifier is the one named amanda, but there are a few others you can try. While the amanda classifier is hidden to the user, the others are located in the autovot/bin/models folder. Alternatively, AutoVOT gives you the option to train your own classifier on labeled data. For more information on training, please visit their main website: https://github.com/mlml/autovot/. Rename AutoVOT output tier autoVOTpostproc.praat After each stop consonant is processed, run autoVOTpostproc.praat to rename the AutoVOT output tier. Otherwise, AutoVOT will overwrite your previous work. Make sure to change the phone name in the script. Return to step 5 and repeat until all 6 stop consonants have been processed.You will need to modify the path to the TextGrids, the tier labels, and the new tier name. Once you have completed this, the cycle starts over until you have all six stops. Move all AutoVOT boundaries to one tier resetBoundaries_stops.praat After running AutoVOT, you should now have a TextGrid with 6 different output tiers: one for each stop consonant. These tiers can be collapsed into one tier with resetBoundaries_stops.praat. You will need to modify the path directory. In addition, if your AutoVOT output does not occupy tiers 3-8, you will need to modify the tier numbers in the script. The final product should look like this: Example output TextGrid from AutoVOT This resembles the previous picture of the TextGrid, but note that the boundaries on the autovot tier are now located at the burst and vocalic onset in the signal. ****If you have manually placed/corrected boundaries, continue on. Otherwise, skip to step 11!**** Stack TextGrids with manual boundaries and the AutoVOT boundaries stackTextGrids.praat This script takes as input TextGrids with manual boundaries (we have two different types) and the AutoVOT TextGrids (_stops.TextGrid). It places all the tiers together in one TextGrid and renames the file _stacked.TextGrid. We have two different TextGrids with manual boundaries in them: _autovot.TextGrid and _check.TextGrid. Using the _autovot TextGrids was meant to eliminate bias from seeing the AutoVOT output. Those TextGrids only contain the window of analysis and not the final measurement. On the other hand, manual adjustments on the _check TextGrids were made directly to the AutoVOT output. This was for efficiency. Only the boundaries on the _autovot files were used for comparison to the AutoVOT output. ****If you want to compare AutoVOT and manual measurements, complete step 9; otherwise, continue on to step 10.**** Compare manual and automatic boundaries measureVOT.praat This script takes as input the _stacked.TextGrid and creates the file manualVOTs.txt. Replace automatic boundaries with manual ones where available resetBoundaries_stacked.praat This script takes as input the _stacked.TextGrids and creates as output _stacked2.TextGrid. Measure VOT and sentence rate cueAnalysis_new.praat This script measures the duration of each burst (positive VOT, if you will), following vowel, and word. It also measures the speaking rate, defined as the average word duration per sentence. The speaking rate component relies on there being two ‘sp’ or silent intervals between each sentence. If your data does not meet this criterion, you can modify the script to fit your data or simply comment out the speaking rate measurement. You will need to modify the path to the wav files and TextGrids. "],["bash-shell-basics.html", "5 Bash Shell Basics 5.1 Introduction 5.2 Familiarization 5.3 Current working directory 5.4 Listing 5.5 Navigating 5.6 Creating directories and files 5.7 Copying files 5.8 Moving files 5.9 Batch renaming and for loops 5.10 Removing files", " 5 Bash Shell Basics 5.1 Introduction In corpus phonetics, the preparation, processing, and analysis of speech data are extensively automated. Use of the command line, or shell, is required for several relevant programs; it also greatly facilitates batch processing of large numbers of files. One of the advantages of corpus phonetics is being able to scale up the size of projects; however, handling large numbers of files individually would be quite tedious. A simple command in the command line can affect all applicable files in one go. Common uses of the command line include moving, renaming, deleting, copying, and creating files. So what exactly is the command line or the shell? It is an interface to your computer’s operating system. It allows the user to directly “speak” to the computer, and navigate through files and folders. By typing a certain command, the user can interact with and modify files, folders, and properties of the computer. Because of this, it is incredibly powerful – many avoid it for this reason, but power is also a good thing, especially when it comes to dealing with large numbers of files. The shell or shells that we’ll be using are the built-in system on Macs: the Terminal, and a comparable program for Windows: Cygwin. These shells both use Unix-based commands, though I’ve been told it’s not technically Unix In this part of the tutorial, we’ll be learning how to use the command line and specifically doing the following: view files and directories (folders) navigate through directories move files copy files rename files safely delete files create files and directories We’ll start off with a brief familiarisation, then go further into navigating and manipulating files through the command line. Much of this part of the tutorial is based heavily on the Software Carpentries course for the Unix Shell, though I’ve inserted several of my own tips and tricks that I’ve also found helpful for corpus phonetics work. 5.2 Familiarization At an abstract level, computers are typically used for a handful of functions including running programs, storing data, and communicating with others. There are several ways in which we can interact with a computer including using a keyboard and mouse, touchscreen, voice command, and so forth. Many of these, and especially the keyboard and mouse and touchscreen variants, use what’s called a graphical user interface (GUI) to interact with the underlying parts of the computer. Breaking apart that term, the display on a Windows or a Mac is simply a visual display of buttons and options that implement commands to the computer when pressed. Companies at least strive to make these interfaces intuitive and easy to interact with (though I’m sure we’ve all encountered some frustrating ones!). Importantly, using a GUI is still an instance of instructing the computer to do something – whether that’s moving a file or opening a program, or creating a file, and so forth. GUIs are great – easy and intuitive, but they often don’t scale very well. While we can sometimes use a select all, copy all sequence, what if we just wanted to select only those files that contained the word “cat” in the filename? Or using a nice example from the Carpentries: “if we have to copy the third line of each of a thousand text files stored in thousand different directories and paste it into a single file line by line.” This would be incredibly tedious and time-consuming. In contrast to the GUI is the command-line interface, or the shell. The The command-line interface is far less visual, but far more powerful, especially for automating well-defined instructions like those just described. For each command entered, the shell reads the command, evaluates and executes it, and prints the output of the command. It then loops back and waits for the next command. This is known as a read-evalaute-print loop. 5.3 Current working directory When opening the shell, the first thing presented on every line is a prompt for input, or the command to be executed. On the Mac terminal, the prompt is a dollar sign, and depending on your settings, the prompt may be preceded by the name of your computer, current location in the computer, and your username. $ # or Eleanor$ # or # computer name: location username$ Eleanors-Macbook-Pro-2:~ Eleanor$ When you first open the shell, it immediately “places you” in your home directory. Remember that you can navigate around the directories on your computer from the shell. This is home base. On Macs the home directory is indicated with a tilde. So what is the full name of our home directory? We can find this out by having the shell spell out the current directory that we’re located in with the command pwd, which stands for print working directory. Eleanors-Macbook-Pro-2:~ Eleanor$ pwd /Users/Eleanor On Windows, this will likely be a location on your C:\\ drive. This command is useful if you ever need a reminder of where you are in the computer. Any command you write will be executed from this location which will be important to remember as we go along. 5.4 Listing The next command we’ll try is one of the most frequent commands used: ls which lists all files and directories in the current working directory. $ ls file1 dir1 file2 dir2 file3 dir3 Note that if you mistype a command, something bad could happen (Ctrl-C is a useful command to remember), or more likely the shell will throw an error indicating that the command was not found: $ ks ks: command not found There are several options or flags you can add to the ls command in order to display additional properties about the files and directories. I’ll mention just a few useful ones here, but as with most things, you can find a complete list online. # show which are files and which directories $ ls -F # show &quot;long&quot; format of file (additional properties like read-write permissions, owner, size, date of creation, etc.) $ ls -l # make the &quot;long&quot; format a bit more human-readable $ ls -lh # show all files in every sub-directory $ ls -R # list files based on time last updated $ ls -t Getting help To see the manual for a command, you can use man $ man ls 5.5 Navigating A key part of the command line is the ability to navigate through the various directories in the computer. This is analagous to clicking on a directory and then a sub-directory and then maybe hitting the back arrow – even using the GUI, we’re also navigating through different locations in the computer. For this to work well, it helps to think of the computer as a massive tree structure of directories – there are root directories, parent directories, and so forth. Using the command cd you can move up, down, and around on that tree structure. Let’s say you just want to go to a directory contained in your current working directory. You can use the following: $ cd name-of-directory # where the name of the directory is a sub-directory of the current working directory Let’s say you want to change to a completely different location on the computer. You can also type in the absolute path to the location starting from your home directory, and this will work from any location on your computer: $ cd /Users/Eleanor/Dropbox/ # where the name of the directory is a sub-directory of the current working directory Note that the presence or absence of the final / typically does not make a difference for the command. Let’s say though that we just want to go to the directory above. We can instead use a relative path to indicate the location. Note that the first example was also an instance of a relative path. To specify a directory above, we use ... $ cd .. We can even go two directories above: $ cd ../.. So really, cd takes as its argument the path, absolute or relative, to the directory you wish to navigate to. How might we go to a directory called corpus that is located one directory above our current working directory? Answer: $ cd ../corpus Another useful shortcut is the dash - character. Using this character brings you back to the last directory you were in – sort of like if you hit the back arrow on your internet browser. This function has nothing to do with the relative path, but rather the history of directories that you have visited. $ cd - Finally, we may not use it today, but another character to be aware of is the single dot .. This refers to the current working directory and is typically left unspecified (not written out) in commands. On occasion, it’s useful to know of its existence $ cd ./corpus 5.6 Creating directories and files To create a directory in the command line, you can use the command mkdir. This takes one argument, the location and name of the new directory. If the location is in the current working directory, as usual, you can just leave out the full path and type the name of the directory. $ mkdir mycorpus $ mkdir mycorpus/audio $ mkdir mycorpus/transcripts Creating files in the command line requires use of a text editor. There are several text editors you can install on the command line, and several come built in, especially on the Terminal. A text editor is like the command line version of Notepad or TextEdit: it’s useful for modifying and creating very simple, basic text files. Note that these editors are indeed restricted to text only: no tables, images, media, etc. We’ll be using an editor called vim. Let’s create a dummy corpus of transcripts so we can practice manipulating and moving files in the command line. $ cd mycorpus # press i to insert This is the transcript for speaker 1. # use arrow keys to navigate around. Can also use option+mouse click to move the cursor long distance. (This also works in the Terminal directly!) # press esc to stop inserting # press wq to write (save) and quit the file. If you need to exit without saving, press q! to forcibly quit Create files for speaker2, speaker3. Maybe we could also just copy a few of these. To view a text file, but not edit, you can use one of the following command: # to show 20 lines at a time $ more speaker1.txt # hit enter/return to see more # q to quit # to show the full file $ cat speaker1.txt # to see the head of a file $ head speaker1.txt # to see the first three (or whatever number of) lines of a file $ head -n 3 speaker1.txt # to see the tail of a file $ tail speaker1.txt # to see the last three (or whatever number of) lines of a file $ tail -n 3 speaker1.txt 5.7 Copying files To copy a file, the command is cp and has two alternative usages: copying a source file to a target file, or copying a set of source files to a target directory. Copying in sense one involves copying one file and creating a new one with a different name. $ cp speaker1.txt speaker4.txt Copying in sense two involves copying one or more files and putting the new versions (with the same name) into a different directory: $ mkdir test $ cp speaker1.txt speaker2.txt test We’ll now need to modify speaker4.txt: $ vi speaker4.txt This is the transcript for speaker (1)4. We can also copy a directory using the recursive -r flag. $ cp -r test test_backup $ ls test test_backup 5.8 Moving files Another very powerful part of the command line is moving files. $ mv source-file location/ $ mv speaker1.txt test/ Moving can also very usefully be used to rename files. Effectively, you move one file into the name of another file. $ mv source-file location/target-file $ mv speaker1.txt speaker1_new.txt $ mv speaker1_new.txt ../speaker1.txt 5.9 Batch renaming and for loops Because this renaming aspect of mv is so useful, we’re going to explore a bit more of what can be done with that. For this part, we’ll introduce regular expressions, for loops, and variables in the shell. To rename all files in a directory, we can do the following. This is simply a command I use frequently in my own scripting, and I’ve found it incredibly useful. I’ll explain each part once we see it in action: $ for i in speaker*.txt; do mv &quot;$i&quot; &quot;${i/.txt/_new.txt}&quot;; done #alternatively $ for i in speaker*.txt &gt; do mv &quot;$i&quot; &quot;${i/.txt/_new.txt}&quot; &gt; done for-loop: iteration over files for-loop structure variables: i –&gt; referred to later with $ and quotes for the string regular expressions and wildcards: very important concept in coding and present in most, if not all programming languages A regular expression is a particular set of characters that define a search pattern. Keyword here is pattern. The * is one of the most common patterns, called the wildcard: match 0 or more characters +: match one preceding character and any number of characters after ?: match any one character in that position ???_speaker: match three characters at the beginning, followed by “_speaker” [a-z] [A-Z] [0-9] [2-5] [a-zA-Z] Curly brackets with three slots within them allow for text substitution: the first slot is the item to undergo a substitution process second slot is the part to be replaced third slot is the replacement Safer implementation Be very careful with mv!! If you’re not sure, make a backup of files and use echo first to print all matches. $ echo speaker*.txt $ for i in speaker*.txt; do echo &quot;$i&quot; &quot;${i/.txt/_new.txt}&quot;; done #alternatively $ for i in speaker*.txt &gt; do echo &quot;$i&quot; &quot;${i/.txt/_new.txt}&quot; &gt; done 5.10 Removing files Removing files can be very risky and so should be done with the utmost care! When you remove files from the command line, the files do not get sent to the Trash bin. Instead, they are immediately and permanently deleted. It is almost impossible to recover these files, so again, use with caution! # to remove with a nice prompt for each file! (highly recommended unless you have hundreds of files you&#39;re deleting at once) $ rm -i speaker1.txt # this will then ask you: remove speaker1.txt? # to which you type either y for yes or n for no y # remember that it will ask you this question for every single file in the list, so if you do: $ rm -i speaker*.txt # and you have 20 files that match speaker*.txt, you will have to type y 20 times to remove them all Alternatively, you can run the quicker but riskier version without the -i flag. $ rm speaker1.txt To remove an entire directory, you’ll need to use the -r flag. This can be combined with the -i flag to give you the ‘are you sure’ warning: $ rm -r myDirectory # or $ rm -ri myDirectory examine files in directory myDirectory? # type y or n: n indicates you want to proceed with deletion # if you type y, then you will see the files and get an extra prompt asking if you want to delete y remove myDirectory? y "],["other-resources.html", "6 Other resources", " 6 Other resources You can still access the legacy pages for older tutorials such as the Montreal Forced Aligner (v1), FAVE-align, and the Penn Forced Aligner. There are many other freely available phonetics tools and resources. I’ve listed just a few of them here. If you know of others that should be added, please let me know! Montreal Forced Aligner - user guide Montreal Forced Aligner - pretrained models Charsiu Forced Aligner Korean Phonetic Aligner Prosodylab Aligner Munich AUtomatic Segmentation System (MAUS) FAVE Montreal Corpus Tools Phonological CorpusTools XPF Corpus Epitran CharsiuG2P VoiceSauce ProsodyPro Phon World Phonotactics Database UCLA Database of Sounds Praat scripts by Matt Winn Praat scripts by Holger Mitterer Praat tutorial by Will Styler Some extra slides I’ve presented related to Corpus Phonetics: Corpus Phonetics Praat Scripting If you have other resources you think might fit in this list and are not yet listed, please send me an email! "]]

[["index.html", "Legacy Pages of the Corpus Phonetics Tutorial 1 Introduction", " Legacy Pages of the Corpus Phonetics Tutorial Eleanor Chodroff Updated: 2022-09-30 1 Introduction These are the legacy tutorials for the first version of the Montreal Forced Aligner, FAVE-align, and the Penn Phonetics Lab Forced Aligner. The documentation for FAVE-align and Penn Forced Aligner is marked as “legacy” as the HTK toolkit these aligners depend on is no longer supported; similarly, the first version of the Montreal Forced Aligner has now been replaced by a very different second version. (I will emphasize here though that FAVE-extract will still work!) FAVE-align Rosenfelder, Ingrid; Fruehwald, Josef; Evanini, Keelan; Seyfarth, Scott; Gorman, Kyle; Prichard, Hilary; Yuan, Jiahong; 2014. FAVE (Forced Alignment and Vowel Extraction) Program Suite v1.2.2 10.5281/zenodo.22281 Penn Phonetics Lab Forced Aligner Yuan, Jiahong., &amp; Liberman, Mark. (2008). Speaker identification on the SCOTUS corpus. In Proceedings of Acoustics, ’08. "],["montreal-forced-aligner-v1-legacy.html", "2 Montreal Forced Aligner v1 (Legacy) 2.1 Overview 2.2 Setup 2.3 Grapheme-to-phoneme models 2.4 Running the aligner 2.5 Tips and tricks", " 2 Montreal Forced Aligner v1 (Legacy) 2.1 Overview Some links in this section may now be broken with the latest update to the Montreal Forced Aligner 2.0 The Montreal Forced Aligner is a forced alignment system with acoustic models built using the Kaldi ASR toolkit. A major highlight of this system is the availability of pretrained acoustic models and grapheme-to-phoneme models for a wide variety of languages. The primary website contains excellent documentation, so I’ll provide some tips and tricks I’ve picked up while using it. A quick link to the installation instructions is located on the primary MFA website. This tutorial is based on Version 1.1. 2.2 Setup As with any forced alignment system, the Montreal Forced Aligner will time-align a transcript to a corresponding audio file at the phone and word levels provided there exist a set of pretrained acoustic models and a lexicon/dictionary of the words in the transcript with their canonical phonetic pronunciation(s). The phone set used in the dictionary must match the phone set in the acoustic models. The orthography used in the dictionary must also match that in the transcript. Very generally, the procedure is as follows: Prep wav file(s) (16 kHz, single channel) Prep transcript(s) (Praat TextGrid or .lab/.txt file) Obtain a pronunciation lexicon Obtain acoustic models You will also need to identify or create an input folder that contains the wav files and TextGrids/transcripts and an output folder for the time-aligned TextGrid to be created. Please make sure that you have separate input and output folders, and that the output folder is not a subdirectory of the input folder! The MFA deletes everything in the output folder: if it is the same as your input folder, the system will delete your input files. 2.2.1 Wav files The Montreal Forced Aligner works best and sometimes will only work with wav files that are sampled at 16 kHz and are single channel files. You may need to resample your audio and extract a single channel prior to running the aligner. prep_audio_mfa.praat 2.2.2 Transcripts The MFA can take as input either a Praat TextGrid or a .lab or .txt file. I have worked most extensively with the TextGrid input, so I’ll describe those details here. As for .lab and .txt input, I have only tried running the aligner where the transcript is pasted in as a single line. I think there is a way of providing timestamps at the utterance level, but I can’t speak to that yet. The most straightforward implementation of the aligner with TextGrid input is to paste the transcript into a TextGrid with a single interval tier. The transcript must be delimited by boundaries on that tier; however, those boundaries cannot be located at either the absolute start or absolute end of the wav file (start boundary != 0, end boundary != total duration). In fact, I’ve found that the MFA can be very sensitive to the location of the end boundary: it’s best to have at least 20 ms, if not 50 ms+ between the final TextGrid boundary and the end of the wav file (see also Section 2.5 on Tips and Tricks). If you have utterance-level timestamps, you can also add in additional intervals for an alignment that is less likely to “derail”. By “derail”, I mean that the aligner gets thrown off early on in the wav file and never gets back on track, which yields a fairly misaligned “alignment”. By delimiting the temporal span of an utterance, the aligner has a chance to reset at the next utterance, even if the preceding utterance was completely misaligned. Side note: misalignments are more likely to occur if there’s additional noise in the wav file (e.g., coughing, background noise) or if the speech and transcript don’t match at either the word or phone level (e.g., pronunciation of a word does not match the dictionary/lexicon entry). Here are few sample Praat scripts I employ for creating TextGrids. If I don’t have timestamps, but I do have a transcript: create_textgrid_mfa_simple.praat If the transcript has start and end times for each utterance (3 column text file with start time, end time, text): create_textgrid_mfa_timestamps.praat That last Praat script can also be modified for a transcript with either start or end times, but not both. Make sure to follow the “rules” (which may change) that text-containing intervals be separated by empty intervals and the boundaries do not align with either the absolute start or end of the file. 2.2.3 Pronunciation lexicon The pronunciation lexicon must be a two column text file with a list of words on the lefthand side and the phonetic pronunciation(s) on the righthand side. Many-to-many mappings between words and pronunciations are permitted. As mentioned above, the phone set must match that used in the acoustic models and the orthography must match that in the transcripts. There are a few options for obtaining a pronunciation lexicon, outlined below. More details about several of these options are in the sections to come. Download the pronunciation lexicon from the MFA website As of writing, there are dictionaries for English, French, and German Generate the pronunciation lexicon from the transcripts using a pretrained grapheme-to-phoneme (G2P) model See section 2.3 on Running a G2P model Train a G2P model to then generate the pronunciation lexicon Create the pronunciation lexicon by hand using the same phone set as the acoustic models 2.2.4 Acoustic models Pretrained acoustic models for several languages can be downloaded from the Montreal Forced Aligner website. If you wish to train custom acoustic models on a speech corpus, this can be accomplished using the Kaldi ASR toolkit. A tutorial for training acoustic models can be found here. 2.3 Grapheme-to-phoneme models If you need a lexicon for the words in your transcript, you might be able to generate one using a grapheme-to-phoneme model. Grapheme-to-phoneme models convert the orthographic representation of a language to its canonical phonetic form after having been trained on examples or conversion rules. Pretrained grapheme-to-phoneme (G2P) models can be found at the Montreal Forced Aligner website. Once you download the one you want, you can follow these instructions: Place grapheme-to-phoneme model in montreal-forced-aligner/pretrained_models folder (they can technically go anywhere, but this structure keeps the files organized) Create input and output folders Place transcripts and wav files in input folder. At least in version 1.1, the wav files needed to be present in order to run the grapheme-to-phoneme conversion model on the transcripts Run grapheme-to-phoneme model cd path/to/montreal-forced-aligner/ bin/mfa_generate_dictionary /path/to/model/file.zip /path/to/corpus /path/to/save.txt bin/mfa_generate_dictionary takes 3 arguments: 1. where is the grapheme-to-phoneme model? 2. where are the wav files and transcripts? (input folder) 3. where should the output go? (output text file) Explicit example (make sure to remove backslashes): cd /Users/Eleanor/montreal-forced-aligner bin/mfa_generate_dictionary pretrained_models/mandarin_character_g2p.zip \\ /Users/Eleanor/Desktop/align_input /Users/Eleanor/Desktop/mandarin_dict.txt 2.4 Running the aligner Place acoustic models and dictionary in montreal-forced-aligner/pretrained_models folder (they can technically go anywhere, but this structure keeps the files organized) Create input and output folders Place TextGrids and wav files in input folder Run Montreal Forced Aligner Make sure to change the arguments of bin/mfa_align! cd path/to/montreal-forced-aligner/ bin/mfa_align corpus_directory dictionary acoustic_model output_directory bin/mfa_align takes 4 arguments: 1. where are the wav files and TextGrids? (input folder) 2. where is the dictionary? 3. where are the acoustic models? (you do need the .zip extension) 4. where should the output go? (output folder) Explicit example (make sure to remove backslashes): cd /Users/Eleanor/montreal-forced-aligner bin/mfa_align /Users/Eleanor/Desktop/align_input pretrained_models/german_dictionary.txt \\ pretrained_models/german.zip /Users/Eleanor/Desktop/align_output 2.5 Tips and tricks Acoustic models You do not need to unzip these. If you do, make sure to call the .zip version. Wav files I mentioned it above, and will mention it again. Things tend to go more smoothly when the wav file is already 16 kHz and a single channel. TextGrids make sure TextGrid boundaries do not align with either the absolute start or end of the file make sure the final TextGrid boundary is at least ~20-50 ms away from the edge (if it still doesn’t work, you might want to try increasing that interval) sometimes it helps to have an empty interval between every interval containing text sometimes it helps to increase the number of intervals present in the file so the aligner becomes less likely to derail "],["fave-align-legacy.html", "3 FAVE-align (Legacy) 3.1 Overview 3.2 Installation 3.3 Running the aligner", " 3 FAVE-align (Legacy) 3.1 Overview What does FAVE do? FAVE is up-to-date implementation of the Penn Forced Aligner covered in section 4. Two programs are included in installation: FAVE-align, which performs forced alignment on American English speech, and FAVE-extract, which performs formant extraction and normalization algorithms. This tutorial focuses on FAVE-align, so future references of FAVE refer to the alignment program only. FAVE takes a sound file and utterance-level transcript (text file) and returns a Praat TextGrid with phone and word alignments. Excellent documentation and instructions for running the aligner can be found on the FAVE-align website. I will cover many of the instructions again, and try to include some additional tips, but provided installation goes smoothly, this aligner tends to work very well right out of the box. What does it include? As in the Penn Forced Aligner, the system comes with pre-trained acoustic models of American English (AE) speech from 25 hours of the SCOTUS corpus, built with the HTK Speech Recognition Toolkit. The section on Kaldi introduces a similar, but alternative system to HTK, in case you’d like to train your own acoustic models from scratch. In addition to the acoustic models, the download also includes a large lexicon of words based on the CMU Pronouncing Dictionary. The dictionary contains over 100,000 words with their standard pronunciation transcriptions in Arpabet. Arpabet is a machine-readable phonetic alphabet of General American English with stress marked on the vowels. While I have, perhaps wrongly, used the Penn Forced Aligner to create phonetic alignments for other languages, those transcriptions had to be forced into Arpabet phones. For a quick alignment, this works fine, but please be advised: the acoustic models of the Penn Forced Aligner are trained on American English, so any speech it is given will be treated like American English. Unless manual adjustments follow the automatic alignment, the PFA alignment would not be ideal for most non-AE phonetic analyses. See the Montreal Forced Aligner sections for pre-trained acoustic models of additional languages and Kaldi if you would like to create custom acoustic models for a particular language or dialect. 3.2 Installation Please refer to the FAVE website for installation. As of writing, that page currently includes links to OS-specific downloads for each of the prerequisites. The prerequisites for FAVE are: HTK Toolkit Version 3.4.1 Unlike p2fa, version 3.4.1 will work If installing HTK on a Mac, there are additional prerequisites you’ll need. Make sure to check out the FAVE wiki for these instructions. Python 2.x Pre-installed on Macs, but you can double-check by opening the terminal and typing python. This will return the version installed. I have been using Version 2.7 and have not had any issues running the aligner. Since you’ve typed python, the terminal is now running with the assumption of python syntax. You want to exit this to get back to its Unix base; to do this, type exit() or quit(). If you have an earlier version of Python, you may need to type exit. SoX Download here: http://sourceforge.net/projects/sox/ To download FAVE, again follow the instructions on the website. FAVE can either be downloaded directly as a .zip or .tar.gz file here or cloned via git using the following command: git clone https://github.com/JoFrhwld/FAVE.git Cloning the system ensures that any new updates are automatically applied to the system. 3.3 Running the aligner FAVE requires a wav file and a corresponding transcript that must adhere to a precise format, which is described below. Unlike some other alignment systems, the wav file does not need to have a sampling rate of 16 kHz. During alignment, the system will automatically create a temporary file with this sampling rate. The text file must be tab-delimited (.txt) with the following five columns: Speaker ID Speaker name Onset time (seconds) Offset time (seconds) Transcription of speech between onset and offset times Some notes I’ve taken that could be useful: The speaker ID can be the same as the speaker name You can make up a very large number as a hack for indicating that the offset time should be the end of the file. The aligner will produce a warning, but still complete the alignment. FAVE sometimes struggles with spontaneous speech, though I imagine any forced alignment system might struggle with this. When the total number of phones in the transcript exceeded the total utterance time under the assumption that a phone is 30 ms, then the aligner produced overlapping intervals. This resulted in areas of Praat TextGrids with overlapping intervals which are visible but no longer functional, and now somewhat useless. It could be a bug in either Praat or FAVE. Some people have asked me how to align multiple files at once. This can be accomplished with a for loop in the shell script that uses regular expression matching to loop through files. Assuming the transcripts and wav files are in the same folder and differ only in their extension (.txt vs .wav), then you could use the following code (make sure to remove the backslashes and run as one line of code): # direct shell to location of FAAValign.py script cd /Users/Eleanor/FAVE/FAVE-align # loop through transcript files in a second directory that contains # the transcript files and like-named wav files for i in /Users/Eleanor/myCorpus/*.txt; \\ do python FAAValign.py &quot;${i/.txt/.wav}&quot; &quot;$i&quot; &quot;${i/.txt/.TextGrid}&quot;; \\ done; "],["penn-forced-aligner-legacy.html", "4 Penn Forced Aligner (Legacy) 4.1 Overview 4.2 Prerequisites 4.3 Modifying the lexicon 4.4 Running the aligner", " 4 Penn Forced Aligner (Legacy) 4.1 Overview What does the Penn Forced Aligner do? The Penn Forced Aligner takes a sound file and the corresponding transcript of speech and returns a Praat TextGrid with phone and word alignments. You can find the website here. What does it include? The system comes with pre-trained acoustic models of American English (AE) speech from the SCOTUS corpus, built with the HTK Speech Recognition Toolkit. The section on Kaldi introduces a similar, but alternative system to HTK, in case you’d like to train your own acoustic models from scratch. In addition to the acoustic models, the download also includes a large lexicon of words based on the CMU Pronouncing Dictionary. The dictionary contains over 100,000 words with their standard pronunciation transcriptions in Arpabet. Arpabet is a machine-readable phonetic alphabet of General American English with stress marked on the vowels. While I have, perhaps wrongly, used the Penn Forced Aligner to create phonetic alignments for other languages, those transcriptions had to be forced into Arpabet phones. For a quick alignment, this works fine, but please be advised: the acoustic models of the Penn Forced Aligner are trained on American English, so any speech it is given will be treated like American English. Unless manual adjustments follow the automatic alignment, the PFA alignment would not be ideal for most non-AE phonetic analyses. Again, see the section on Kaldi if you would like to create acoustic models for a different language or dialect. 4.2 Prerequisites HTK Toolkit Version 3.4 Version 3.4.1 will not work Download here: http://htk.eng.cam.ac.uk/ Many run into issues installing this, but detailed installation instructions (and additional tutorial notes) can be found here. Update as of November 11, 2018: this website (linguisticmystic.com) no longer works, but covered how to install the Penn Forced Aligner on a Mac. The Penn Forced Aligner is no longer being maintained, and has instead been replaced by FAVE (section 3). The corresponding prerequisites for HTK installation on Mac are now covered on the FAVE wiki). Python 2.5 or 2.6 Pre-installed on Macs, but you can double-check by opening the terminal and typing python. This will return the version installed. I have been using Version 2.7.6 and have not had any issues running the aligner. Since you’ve typed python, the terminal is now running with the assumption of python syntax. You want to exit this to get back to its Unix base; to do this, type exit() or quit(). If you have an earlier version of Python, you may need to type exit. SoX Download here: http://sourceforge.net/projects/sox/ 4.3 Modifying the lexicon After downloading the Penn Forced Aligner, you should now have a directory named p2fa. Before beginning any of the following steps, take note of the parent directory and path, as you will need to direct your terminal to the exact p2fa location. For example, my p2fa directory is located in /Users/Eleanor. I can direct my terminal now to that location and view the contents of that directory by typing the following: cd /Users/Eleanor/ ls Once you’ve located the p2fa directory, you can find the lexicon in p2fa/model/dict. Despite the lack of a .txt extension, dict is a text file, making it quite easy to add words and non-words alike. You’ll want to make sure that all words in your transcript are indeed in the dictionary. This can be done by converting your transcript into a word list and comparing it against the dictionary. First, you’ll need a copy of your transcript in a text file called fulltranscript.txt. (Actually, you can call it whatever you want; just make sure to change the name in the code!) In the terminal, navigate to the location of your transcript and then we can use the long code to create a list of all unique words in the transcript. My fulltranscript.txt is located in /Users/Eleanor/myCorpus. You’ll need to change that part to match your file location. cd /Users/Eleanor/myCorpus tr &#39; &#39; &#39;\\n&#39; &lt; fulltranscript.txt | tr &#39;[a-z]&#39; &#39;[A-Z]&#39; | \\ sed &#39;/^$/d&#39; | sed &#39;/[.,?!;:]/d&#39; | sort | uniq -c | sed &#39;s/^ *//&#39; | \\ sort -r -n &gt; fulltranscript_words.txt The above clearly does a whole slew of functions. It will first take your transcript, separate each word with a new line (tr ' ' '\\n' &lt; fulltranscript.txt), capitalize all letters (tr '[a-z]' '[A-Z]'), delete blank lines (sed '/^$/d'), remove punctuation (sed '/[.,?!;:]/d'), sort the words (sort), remove duplicate words (uniq -c), delete blank lines/trailing white space again (sed 's/^ *//'), sort again and give you a count of how many times each word appears in the transcript (sort -r -n &gt; fulltranscript_words.txt). The following code will find the word pronunciations in the CMU dictionary. This is accomplished by taking the words and putting them into the regular expression format for locating the beginning of a line (^). This results in tmp.txt. That file is then compared against the CMU dictionary. If the word is in the dictionary, then the dictionary line is extracted such that you have both the word and its pronunciation. Note that when using the cut command, the default cut is tab (cut -f 2), but if the delimiter is anything other than tab (space, comma, etc.), it can be specified with cut -d 'my delimiter' -f 2 myfile.txt. cut -d &#39; &#39; -f 2 fulltranscript_**words**.txt | sed &#39;s/^/^/&#39; | sed &#39;s/$/ /&#39; &gt; tmp.txt egrep --file=tmp.txt /Users/Eleanor/p2fa/model/dict &gt; fulltranscript_words_pron.txt; You then need to determine which words were skipped in this process, i.e., the words missing from the CMU dictionary. This can be done by comparing the final fulltranscript_words_pron.txt against the original fulltranscript_words.txt. # extracts and sorts relevant columns and stores in tmp file sort -k 2 fulltranscript_words.txt &gt; tmp1.txt sort -k 1 fulltranscript_words_pron.txt &gt; tmp2.txt # merges the two files to list words, word count, and pron join -1 2 -2 1 tmp1.txt tmp2.txt &gt; fulltranscript_words_pron2.txt # lists words with missing pronunciations # these are the words you need to add to the dictionary join -v 1 -1 2 -2 1 tmp1.txt tmp2.txt &gt; fulltranscript_words_missing_pron.txt # Extras: # get count of word types wc -l &lt; fulltranscript_words.txt # get count of phone types wc -l &lt; fulltranscript_words_pron.txt # get count of phone tokens cut -d&#39; &#39; -f 3- &lt; fulltranscript_words_pron.txt | tr &#39; &#39; &#39;\\n&#39; | sort | uniq -c | \\ sed &#39;s/^ *//&#39; | sort -r -n &gt; fulltranscript_phones.txt Lexical entries need to be added in the same format as the rest of the dictionary, which is the word in all caps followed by two spaces and the CMU pronunciation with stress on the vowel. For example: KLATT K L AE1 T ELEANOR EH1 L AH0 N AO2 R ELEANOR EH1 L AH0 N ER2 Multiple pronunciation variants are fine (this can even be used to test some interesting hypotheses). CMU provides a nice tool for converting standard spellings of words and non-words into the correct pronunciation. This can be found here: http://www.speech.cs.cmu.edu/tools/lextool.html. After running this tool, you will need to add the stress value to the vowels (1 = primary, 0 = unstressed, 2 = secondary). You can always refer to similar words in the dictionary for an example. 4.4 Running the aligner The standard implementation of the Penn Forced Aligner will process a single wav file and transcript, returning the phone and word alignment on a Praat TextGrid. Before beginning this part of the tutorial, make sure that all the words in your transcript are in the included lexicon, p2fa/model/dict. If you’re not sure or know that they are not, please visit Section 4.3. Ingredients: * Wav file of recording * Corresponding transcript. Example below: SAY TUTT AGAIN SAY PAT AGAIN SAY DOT AGAIN The transcript should contain the words with spaces between them; these are standardly capitalized, but the aligner will accept lowercase and uppercase letters. Line breaks are fine, as are apostrophes, as long as that spelling is in the dictionary. All punctuation should be removed. Others have suggested adding an “sp” or space between words and sentences. This is not necessary. The aligner will determiner whether or not a small space or silence is present. An important note is that the aligner can derail if there is untranscribed noise or speech in the recording. In my experience, it’s not too bad at recovering after a short while, but it’s best to avoid that situation. This can be accomplished by giving it smaller portions of the wav file, or ensuring that all noise and extraneous speech is explicitly transcribed. Smaller portions of the wav file can be created manually by extracting the relevant clip(s). Alternatively, a modified version of the script can process relevant sections of speech defined by their start and end times. Yet another option is to transcribe noise as {NS} and silence as {SP}. I have not tried this method, so I do not know how robust it is to extraneous speech. To process a single wav file and transcript, direct the terminal to the directory containing the Penn Forced Aligner align.py script with cd, then type the second command. The arguments to the align.py script are the locations of the wav file and transcript file. The script creates the aligned TextGrid as output (subj01.TextGrid, but you can call it whatever you want). cd ~/p2fa python align.py /Users/Eleanor/myCorpus/subj01.wav \\ /Users/Eleanor/myCorpus/subj01.txt subj01.TextGrid And that’s it! "]]
